{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a> 1) Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T07:59:11.052753Z",
     "iopub.status.busy": "2021-10-18T07:59:11.051989Z",
     "iopub.status.idle": "2021-10-18T07:59:11.058593Z",
     "shell.execute_reply": "2021-10-18T07:59:11.057919Z",
     "shell.execute_reply.started": "2021-10-18T07:59:11.052712Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# for building DQN model\n",
    "\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "# from Env_v1 import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a>2) Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T08:00:15.545561Z",
     "iopub.status.busy": "2021-10-18T08:00:15.545283Z",
     "iopub.status.idle": "2021-10-18T08:00:15.560218Z",
     "shell.execute_reply": "2021-10-18T08:00:15.559564Z",
     "shell.execute_reply.started": "2021-10-18T08:00:15.545513Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "# Time_matrix = np.load(\"TM.npy\")\n",
    "# Loading the time matrix (part of the sample code)\n",
    "Time_matrix = np.load(\"../input/cab-driverdeep-rl/TM.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T08:00:18.405433Z",
     "iopub.status.busy": "2021-10-18T08:00:18.404889Z",
     "iopub.status.idle": "2021-10-18T08:00:18.418485Z",
     "shell.execute_reply": "2021-10-18T08:00:18.417560Z",
     "shell.execute_reply.started": "2021-10-18T08:00:18.405386Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Time_matrix[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T08:00:24.187235Z",
     "iopub.status.busy": "2021-10-18T08:00:24.186392Z",
     "iopub.status.idle": "2021-10-18T08:00:24.195702Z",
     "shell.execute_reply": "2021-10-18T08:00:24.193287Z",
     "shell.execute_reply.started": "2021-10-18T08:00:24.187187Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The shape of Time Matrix is:\",Time_matrix.shape)\n",
    "print(\"The max travel duration encountered in a journey:\",Time_matrix.max())\n",
    "print(\"The min travel duration observed:\",Time_matrix.min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<A> The maximum travel duration encountered during a journey is 11 hours(less than 24 hrs). Thus the next state of the cab driver may be updated by just 1 hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T08:01:14.623114Z",
     "iopub.status.busy": "2021-10-18T08:01:14.622701Z",
     "iopub.status.idle": "2021-10-18T08:01:14.651208Z",
     "shell.execute_reply": "2021-10-18T08:01:14.650475Z",
     "shell.execute_reply.started": "2021-10-18T08:01:14.623077Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import routines\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from matplotlib import style\n",
    "import time\n",
    "\n",
    "\n",
    "# Defining hyperparameters\n",
    "m = 5 # number of cities, ranges from 0 ..... m-1\n",
    "t = 24 # number of hours, ranges from 0 .... t-1\n",
    "d = 7  # number of days, ranges from 0 ... d-1\n",
    "C = 5 # Per hour fuel and other costs\n",
    "R = 9 # per hour revenue from a passenger\n",
    "\n",
    "\n",
    "\n",
    "class CabDriver():\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"initialise your state and define your action space and state space\"\"\"\n",
    "        self.action_space = [(p,q) \n",
    "                             for p in range(m) for q in range(m) if p != q or p == 0]\n",
    "        self.state_space = [(x,time,day) \n",
    "                            for x in range(m) for time in range(t) for day in range(d)]\n",
    "\n",
    "        self.state_init = random.choice(self.state_space)\n",
    "        self.action_init = random.choice(self.action_space)\n",
    "        self.poisson_dist = [2, 12, 4, 7, 8]\n",
    "\n",
    "        # Starting the first round\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    ## Encoding state (or state-action) for NN input\n",
    "\n",
    "    def state_encod_arch1(self, state):\n",
    "        \"\"\"convert the state into a vector so that it can be fed to the NN. This method converts a given state into a vector format. Hint: The vector is of size m + t + d.\"\"\"\n",
    "        # Flatten the state as a (5+24+7) len vector with 3 hot values representing loc, time and day\n",
    "        # Action will be the output of Q-NN\n",
    "        # Total len of first layer input = 36\n",
    "        state_encod = [0 for i in range(m + t + d)]\n",
    "        state_encod[state[0]] = 1         # Location       \n",
    "        state_encod[m + state[1]] = 1       # Time of the day\n",
    "        state_encod[m+t + state[2]] = 1     # Day of the week\n",
    "        return state_encod\n",
    "\n",
    "\n",
    "    # Use this function if you are using architecture-2 \n",
    "    def state_encod_arch2(self, state, action):\n",
    "        \"\"\"convert the (state-action) into a vector so that it can be fed to the NN. This method converts a given state-action pair into a vector format. Hint: The vector is of size m + t + d + m + m.\"\"\"\n",
    "        # Flatten the state & the action also in this architecture\n",
    "        # state is flattened as a (m+t+d = 5+24+7) len vector with 3 hot values representing loc, time and day \n",
    "        # Action can be encoded as (m+m) len vector - first m for 'from' the 2nd for 'to'. None is activated for No Ride\n",
    "        # Total len of first layer input = 5+24+7+5+5 = 46\n",
    "        state_encod = [0 for i in range(m + t + d + m + m)]\n",
    "        state_encod[state[0]] = 1         # Location       \n",
    "        state_encod[m + state[1]] = 1       # Time of the day\n",
    "        state_encod[m+t + state[2]] = 1     # Day of the week\n",
    "        if action != (0,0):\n",
    "            state_encod[m+t+d+ action[0]] = 1    #From\n",
    "            state_encod[m+t+d+m+ action[1]] = 1  #To\n",
    "            \n",
    "        return state_encod\n",
    "\n",
    "\n",
    "    ## Getting number of requests\n",
    "\n",
    "    def requests(self, state):\n",
    "        \"\"\"Determining the number of requests basis the location. \n",
    "        Use the table specified in the MDP and complete for rest of the locations\"\"\"\n",
    "        location = state[0]\n",
    "        requests = np.random.poisson(self.poisson_dist[location-1])\n",
    "            \n",
    "        # Capping the requests to 15\n",
    "\n",
    "        if requests >15:\n",
    "            requests =15\n",
    "\n",
    "        possible_actions_index = random.sample(range(1, (m-1)*m +1), requests) # (0,0) is not considered as customer request\n",
    "        actions = [self.action_space[i] for i in possible_actions_index]\n",
    "        \n",
    "        # Let us append the no ride conditon\n",
    "        actions.append([0,0])\n",
    "        \n",
    "        possible_actions_index.append(self.action_space.index((0,0)))\n",
    "\n",
    "        return possible_actions_index,actions   \n",
    "\n",
    "\n",
    "\n",
    "    def reward_func(self, state, action, Time_matrix):\n",
    "        \"\"\"Takes in state, action and Time-matrix and returns the reward\"\"\"\n",
    "        next_state, wasted_time, travel_time, ride_time = self.next_state_func(state, action, Time_matrix)\n",
    "        \n",
    "        revenue_time = ride_time\n",
    "        fuel_spent = travel_time + ride_time\n",
    "        \n",
    "        if (action[0] == 0 and action[1] == 0):\n",
    "            reward = - C\n",
    "        else:\n",
    "            reward = R * (revenue_time) - C * (revenue_time + fuel_spent)\n",
    "         \n",
    "        return reward\n",
    "\n",
    "    def updated_day_time(self, time, day):\n",
    "        # correcting the time for 24 hour format\n",
    "        if  time > 23:\n",
    "            new_hour = time - 24\n",
    "            new_day = day + 1\n",
    "            if new_day > 6:\n",
    "                new_day = new_day - 7\n",
    "            else:\n",
    "                new_day = new_day\n",
    "        else:\n",
    "            new_hour = time\n",
    "            new_day = day\n",
    "        return new_day, new_hour\n",
    "\n",
    "\n",
    "    def next_state_func(self, state, action, Time_matrix):\n",
    "#         self.episode_step += 1\n",
    "        \"\"\"Takes state and action as input and returns next state\"\"\"\n",
    "        curr_loc  = state[0]\n",
    "        curr_hour = state[1]\n",
    "        curr_day  = state[2]\n",
    "        pickup_loc= action[0]\n",
    "        drop_loc  = action[1]\n",
    "        \n",
    "        # let us initialise time\n",
    "        wasted_time = 0\n",
    "        travel_time = 0\n",
    "        ride_time = 0\n",
    "        \n",
    "        # Travelling time from current position to the pick up location\n",
    "        ## 1) In case the driver rejects the ride\n",
    "        if (pickup_loc) == 0 and (drop_loc == 0):  \n",
    "            updated_loc  = curr_loc         # His location doesnt update as ride is denied\n",
    "            wasted_time = int(1)               # He has to wait for a hour for next ride\n",
    "            new_hour = curr_hour + wasted_time    \n",
    "            \n",
    "            updated_day, updated_hour = self.updated_day_time(new_hour, curr_day)\n",
    "            \n",
    "        ## 2) In case the ride is accepted\n",
    "        else:\n",
    "             # Using Time matrix\n",
    "            \n",
    "            travel_time = Time_matrix[curr_loc][pickup_loc][curr_hour][curr_day]\n",
    "            new_hour = int(curr_hour + travel_time)  #converting into int\n",
    "\n",
    "            updated_day, updated_hour = self.updated_day_time(new_hour, curr_day)\n",
    "                \n",
    "            updated_loc = drop_loc\n",
    "            # Retrieving time taken for the updated parameters\n",
    "            ride_time = Time_matrix[pickup_loc][updated_loc][updated_hour][updated_day]\n",
    "            \n",
    "        next_state  = [updated_loc, updated_hour,updated_day]\n",
    "        return next_state, wasted_time, travel_time, ride_time\n",
    "    \n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "#         self.episode_step = 0\n",
    "        return self.action_space, self.state_space, self.state_init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a>3)  Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Own Tensorboard class\n",
    "# class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "#     # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.step = 1\n",
    "        \n",
    "#         self.writer = tf.summary.create_file_writer(self.log_dir)\n",
    "\n",
    "#     # Overriding this method to stop creating default log writer\n",
    "#     def set_model(self, model):\n",
    "#         pass\n",
    "\n",
    "#     # Overrided, saves logs with our step number\n",
    "#     # (otherwise every .fit() will start writing from 0th step)\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         self.update_stats(**logs)\n",
    "\n",
    "#     # Overrided\n",
    "#     # We train for one batch only, no need to save anything at epoch end\n",
    "#     def on_batch_end(self, batch, logs=None):\n",
    "#         pass\n",
    "\n",
    "#     # Overrided, so won't close writer\n",
    "#     def on_train_end(self, _):\n",
    "#         pass\n",
    "\n",
    "#     # Custom method for saving own metrics\n",
    "#     # Creates writer, writes custom metrics and closes writer\n",
    "#     def update_stats(self, **stats):\n",
    "#         self._write_logs(stats, self.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = '1st Architecture'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T08:02:09.264867Z",
     "iopub.status.busy": "2021-10-18T08:02:09.264600Z",
     "iopub.status.idle": "2021-10-18T08:02:09.291115Z",
     "shell.execute_reply": "2021-10-18T08:02:09.290305Z",
     "shell.execute_reply.started": "2021-10-18T08:02:09.264839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, env):\n",
    "        # Define size of state and action\n",
    "        self.state_size  = state_size\n",
    "        self.action_size = action_size\n",
    "        self.env = env\n",
    "#         self.action_tracked = action_tracked\n",
    "        \n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.DISCOUNT = 0.95\n",
    "        self.LR  = 0.001  # Learning Rate\n",
    "        \n",
    "        # Exploration settings\n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon = self.epsilon_max\n",
    "        self.EPSILON_DECAY = 0.0009\n",
    "        self.MIN_EPSILON = 0.00001\n",
    "\n",
    "\n",
    "        # create replay memory using deque\n",
    "        self.REPLAY_MEMORY_SIZE = 2_000\n",
    "        self.replay_memory = deque(maxlen=2_000)  # How many last steps to keep for model training\n",
    "#         self.MIN_REPLAY_MEMORY_SIZE = 100 # Minimum number of steps in a memory to start training\n",
    "        self.MINIBATCH_SIZE = 32  # How many steps (samples) to use for training\n",
    "        \n",
    "#         self.MODEL_NAME = '1st Architecture'\n",
    "#         self.tensorboard = ModifiedTensorBoard(log_dir=f\"logs/{MODEL_NAME}-{int(time.time())}\")\n",
    "        \n",
    "        # Initialize the value of the states tracked\n",
    "        self.states_tracked = []\n",
    "        \n",
    "        # We are going to track state [1,1,1]\n",
    "        self.track_state = np.array(env.state_encod_arch1([1,1,1])).reshape(1, 36)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "    \n",
    "        \n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        '''\n",
    "        TODO:\n",
    "        Build multilayer perceptron to train the Q(s,a) function. In this neural network, the input will be states and the output \n",
    "        will be Q(s,a) for each (state,action). \n",
    "        Note: Since the ouput Q(s,a) is not restricted from 0 to 1, we use 'linear activation' as output layer.\n",
    "\n",
    "        Loss Function:\n",
    "        Loss=1/2 * (R_t + γ∗max Q_t (S_{t+1},a)−Q_t(S_t,a)^2\n",
    "               which is 'mean squared error'\n",
    "\n",
    "        '''\n",
    "        model = Sequential()            \n",
    "        # Write your code here: Add layers to your neural nets   \n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        ...\n",
    "        model.add(Dense(32,activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        ...\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))     # action_size = how many choices (21)\n",
    "        ...\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.LR))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def get_action(self, state,poss_rides_index, action_space):\n",
    "        '''\n",
    "        Select action\n",
    "        Args:\n",
    "            state: At any given state, choose action\n",
    "        \n",
    "        TODO:\n",
    "        Choose action according to ε-greedy policy. We generate a random number over [0, 1) from uniform distribution.\n",
    "        If the generated number is less than ε, we will explore, otherwise we will exploit the policy by choosing the\n",
    "        action which has maximum Q-value.\n",
    "        \n",
    "        More the ε value, more will be exploration and less exploitation.\n",
    "        \n",
    "        '''\n",
    "           \n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            #randomly choosing an action from feasible \"ride requests\"\n",
    "            \n",
    "            rand_index = random.randrange(len(poss_rides_index))\n",
    "            action = action_space[rand_index]\n",
    "            return rand_index, action\n",
    "\n",
    "        else:\n",
    "            # lets convert the encoded state into an array and reshape it in the correct format-(batch_size,input_shape)\n",
    "            state_vector = np.array(env.state_encod_arch1(state)).reshape(1, self.state_size)\n",
    "            q_val = self.model.predict(state_vector)\n",
    "            max_index = np.argmax(q_val[0])\n",
    "            action = action_space[max_index]\n",
    "            return max_index, action\n",
    "    \n",
    "    \n",
    "    def update_replay_memory(self, state, action, reward, next_state, done):\n",
    "        # save sample <s,a,r,s'> to the replay memory\n",
    "\n",
    "        # Adding sample to the memory. \n",
    "        self.replay_memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "#                 # Decay in ε after we generate each sample from the environment\n",
    "#         if self.epsilon > self.MIN_EPSILON:\n",
    "#             self.epsilon *= self.EPSILON_DECAY\n",
    "        \n",
    "    \n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        '''\n",
    "        Train the neural network to find the best policy\n",
    "        \n",
    "        TODO:\n",
    "        1. Sample <s,a,r,s',done> of batch size from the memory\n",
    "        2. Set the target as R_t + γ∗max Q_t(S_{t+1},a)−Q_t(S_t,a)\n",
    "        3. We already have the actions that we took when generating sample from environment\n",
    "        4. To find the Q_t(S_t,a), we input the current state s to the model, and we get Q-value for all the actions\n",
    "        5. To find the Q_t(S_{t+1},a), we input the next state s' to the model, and we get Q-value for all the actions\n",
    "        6. Train the model        \n",
    "        \n",
    "        '''\n",
    "        if len(self.replay_memory) > self.MINIBATCH_SIZE:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.replay_memory, self.MINIBATCH_SIZE)\n",
    "            # initialise two matrices - update_input and update_output\n",
    "            update_input = np.zeros((self.MINIBATCH_SIZE, self.state_size))\n",
    "            update_output = np.zeros((self.MINIBATCH_SIZE, self.state_size))\n",
    "            actions, rewards, done = [], [], []\n",
    "\n",
    "            # populate update_input and update_output and the lists rewards, actions, done\n",
    "            for i in range(self.MINIBATCH_SIZE):\n",
    "                state, action, reward, next_state, done_boolean = mini_batch[i]\n",
    "                update_input[i] = env.state_encod_arch1(state)     \n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                update_output[i] = env.state_encod_arch1(next_state)\n",
    "                done.append(done_boolean)\n",
    "\n",
    "            # predict the target q-values from states s\n",
    "            current_qval = self.model.predict(update_input)\n",
    "            # target for q-network\n",
    "            future_qval = self.model.predict(update_output)\n",
    "\n",
    "\n",
    "            # update the target values\n",
    "            for i in range(self.MINIBATCH_SIZE):\n",
    "                if done[i]:\n",
    "                    current_qval[i][actions[i]] = rewards[i]\n",
    "                else: # non-terminal state\n",
    "                    current_qval[i][actions[i]] = rewards[i] + self.DISCOUNT * np.max(future_qval[i])\n",
    "                    \n",
    "            self.model.fit(update_input, current_qval, batch_size=self.MINIBATCH_SIZE)\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        self.model.save_weights(\"weights.h5\")\n",
    "        self.model.save(\"my_model.pkl\")\n",
    "        \n",
    "        \n",
    "                \n",
    "    def save_qval_for_tracked_state(self):\n",
    "        # Use the model to predict the q_value of the state we are tacking.\n",
    "        q_value = self.model.predict(self.track_state)\n",
    "        \n",
    "        # Let us track the q_value for some randomly selected \"action index\"\n",
    "        # Let us select action index 5 i.e (1,0)\n",
    "        self.states_tracked.append(q_value[0][5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a> 4) DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T08:16:22.146864Z",
     "iopub.status.busy": "2021-10-18T08:16:22.146494Z",
     "iopub.status.idle": "2021-10-18T08:16:22.196022Z",
     "shell.execute_reply": "2021-10-18T08:16:22.195355Z",
     "shell.execute_reply.started": "2021-10-18T08:16:22.146827Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Episodes = 3000\n",
    "episode_length = 720  # car discharges after this period\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Accumulating rewards from every epsiode and keeping track of episodes \n",
    "rewards_per_episode, episodes, avg_rewards_per_episode = [], [], []\n",
    "\n",
    "# Invoke Env class\n",
    "env = CabDriver()\n",
    "\n",
    "# get size of state and action from environment\n",
    "state_size = len(env.state_encod_arch1(env.state_init))\n",
    "action_size = len(env.action_space)\n",
    "\n",
    "#Call the DQN agent\n",
    "agent = DQNAgent(state_size, action_size,env)\n",
    "action_space = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T08:16:23.806107Z",
     "iopub.status.busy": "2021-10-18T08:16:23.805858Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for episode in tqdm(range(1,Episodes +1), ascii = True, unit = 'episode'):\n",
    "for episode in range(Episodes +1):\n",
    "    # Update tensorboard step every episode\n",
    "#     agent.tensorboard.step = episode\n",
    "\n",
    "    # Invoke Env class\n",
    "    env = CabDriver()\n",
    "    \n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    reward_per_timestep = 0\n",
    "    time_step = 0\n",
    "\n",
    "    # Reset environment and get initial state\n",
    "    action_space, state_space, state = env.reset()\n",
    "#     current_state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Rnadomly Initialise a state & an action for tracking q-value \n",
    "    state_to_be_tracked = env.state_init\n",
    "#     action_tracked = env.action_init\n",
    "\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    total_journey = 0\n",
    "    \n",
    "    while not done:\n",
    "        time_step +=1\n",
    "        # 1. Get a possible list of the ride requests driver gets\n",
    "        possible_actions_index,actions = env.requests(state)\n",
    "        ...\n",
    "        # 2. Selectig an action from Epsilon greedy policy\n",
    "        input_state = env.state_encod_arch1(state)\n",
    "#         action_space = env.action_space\n",
    "        action_index, action = agent.get_action(input_state, possible_actions_index, action_space)\n",
    "        ...\n",
    "        # 3. Evaluating the next state \n",
    "        next_state, wasted_time, travel_time, ride_time = env.next_state_func(state, action,\n",
    "                                                                             Time_matrix)\n",
    "        # 4. Evaluating reward for every time step\n",
    "        reward = env.reward_func(state, action, Time_matrix)   \n",
    "        ...\n",
    "         # 5.Calculating total journey time in the episode\n",
    "        total_journey += wasted_time + travel_time + ride_time\n",
    "        \n",
    "        if total_journey >= episode_length:\n",
    "            print(\"Episode Terminated\")\n",
    "            done = True         \n",
    "        \n",
    "        # 5. Every step we update replay memory \n",
    "        agent.update_replay_memory(env.state_encod_arch1(state), action_index,\n",
    "                            reward, env.state_encod_arch1(next_state), done)\n",
    "#         # 5. Proceeding ahead only if the episode hasnt ended\n",
    "#         if done == False:\n",
    "\n",
    "        # 7. Train main network\n",
    "        if time_step % 20 == 0:\n",
    "                agent.train_model()\n",
    "                \n",
    "         # 8. Tracking reward & updating the state\n",
    "        reward_per_timestep += reward\n",
    "        current_state = next_state            \n",
    "        \n",
    "        # 9. check for terminal state\n",
    "        if done == True:\n",
    "            print(\"Episode Terminated\")\n",
    "            terminal_state = True\n",
    "            \n",
    "    # 10. Evaluating total reward obtained in this entire episode\n",
    "    rewards_per_episode.append(reward_per_timestep)\n",
    "    episodes.append(episode)\n",
    "    \n",
    "    # 11. epsilon decay\n",
    "    if agent.epsilon > agent.MIN_EPSILON:\n",
    "        agent.epsilon = agent.MIN_EPSILON + (agent.epsilon_max - agent.MIN_EPSILON) * np.exp(-agent.EPSILON_DECAY*episode)\n",
    "    \n",
    "    # 10. TRACKING REWARDS\n",
    "    # every 10 episodes:\n",
    "#     if ((episode) % 20 == 0):\n",
    "    print(\"episode: {0}, reward: {1}, memory_length: {2}, epsilon: {3} \".format(episode,\n",
    "                                                                         reward_per_timestep,\n",
    "                                                                         len(agent.replay_memory),\n",
    "                                                                         agent.epsilon))\n",
    "        \n",
    "    # Save the Q_value of the state, action pair we are tracking\n",
    "    if ((episode) % 10 == 0):\n",
    "        agent.save_qval_for_tracked_state()\n",
    "\n",
    "#     # Total rewards per episode\n",
    "#     reward_tracked.append(reward)\n",
    "    \n",
    "# print(\"Saving Model and weights {}\".format(episode))\n",
    "# agent.save()\n",
    "    \n",
    "    if(episode % 1000 == 0):\n",
    "        print(\"Saving Model and weights {}\".format(episode))\n",
    "        agent.save()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = time.time() - start_time\n",
    "print('Total time taken ',elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_qval_for_tracked_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence <br>\n",
    "\n",
    "There are two ways to check the convergence of the DQN model:\n",
    "\n",
    " - Sample a few state-action pairs and plot their Q-values along episodes\n",
    "\n",
    " - Check whether the total rewards earned per episode are showing stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Convergence by tracking total rewards per episode vs episode number\n",
    "plt.plot(list(range(len(rewards_per_episode))), rewards_per_episode)\n",
    "plt.ylabel(\"Total rewards\")\n",
    "plt.title('Rewards per episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us track Average reward per 50 episode\n",
    "avg_rewards = []\n",
    "episodes = len(rewards_per_episode)\n",
    "index = 0\n",
    "track_total_reward = 0\n",
    "for episode_number in range(episodes):\n",
    "    if index != 50:\n",
    "        track_total_reward += rewards_per_episode[episode_number]\n",
    "        index += 1\n",
    "    else:\n",
    "        avg_rewards.append(track_total_reward/index)\n",
    "        track_total_reward = rewards_per_episode[episode_number]\n",
    "        index = 1\n",
    "\n",
    "avg_rewards.append(track_total_reward/index)\n",
    "        \n",
    "    \n",
    "print(avg_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check Convergence by tracking average rewards per episode vs episode number\n",
    "plt.plot(list(range(len(avg_rewards))), avg_rewards)\n",
    "plt.ylabel(\"Average rewards\")\n",
    "plt.title('Average Rewards vs Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "plt.title('Q_value for state [1,1,1]  action (1,0)')\n",
    "xaxis = np.asarray(range(0, len(agent.states_tracked)))\n",
    "plt.semilogy(xaxis,np.asarray(agent.states_tracked))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# state_tracked_sample = [agent.states_tracked[i] for i in range(len(agent.states_tracked)) if agent.states_tracked[i] < 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runnable example\n",
    "sequential_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(784,), name=\"digits\"),\n",
    "        keras.layers.Dense(64, activation=\"relu\", name=\"dense_1\"),\n",
    "        keras.layers.Dense(64, activation=\"relu\", name=\"dense_2\"),\n",
    "        keras.layers.Dense(10, name=\"predictions\"),\n",
    "    ]\n",
    ")\n",
    "sequential_model.save_weights(\"weights.h5\")\n",
    "sequential_model.load_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling `save('my_model.h5')` creates a h5 file `my_model.h5`.\n",
    "model.save(\"my_h5_model.h5\")\n",
    "\n",
    "# It can be used to reconstruct the model identically.\n",
    "reconstructed_model = keras.models.load_model(\"my_h5_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
    "model.save(\"my_model\")\n",
    "\n",
    "# It can be used to reconstruct the model identically.\n",
    "reconstructed_model = keras.models.load_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
